{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1. 2.]\n",
      "Function value at minimum: 0.0\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton_method(f, grad_f, hess_f, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Newton's method for unconstrained scalar function optimization.\n",
    "\n",
    "    Parameters:\n",
    "        f (callable): Objective function.\n",
    "        grad_f (callable): Gradient of the objective function.\n",
    "        hess_f (callable): Hessian of the objective function.\n",
    "        x0 (numpy array): Initial guess.\n",
    "        tol (float): Tolerance for convergence (gradient norm).\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        x (numpy array): Optimized variable.\n",
    "        f_val (float): Value of the function at the minimum.\n",
    "        iter_count (int): Number of iterations performed.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    iter_count = 0\n",
    "    g = grad_f(x)\n",
    "    while np.linalg.norm(g) > tol and iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "\n",
    "        # Evaluate gradient and Hessian\n",
    "        \n",
    "        H = hess_f(x)\n",
    "\n",
    "        # Compute Newton step (ensure Hessian is invertible)\n",
    "        try:\n",
    "            delta_x = np.linalg.solve(H, -g)  # Solve H * delta_x = -g\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian is singular or not invertible. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Update x\n",
    "        x = x + delta_x\n",
    "        g = grad_f(x)\n",
    "\n",
    "    return x, f(x), iter_count\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the function and its gradient\n",
    "    def f(x):\n",
    "        return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "    def grad_f(x):\n",
    "        return np.array([2 * (x[0] - 1), 2 * (x[1] - 2)])\n",
    "    \n",
    "    # Hessian as a NumPy array\n",
    "    def hess_f(x):\n",
    "        return np.array([[2, 0], [0, 2]])  # Identity matrix for the quadratic function\n",
    "\n",
    "    # Initial guess\n",
    "    x0 = np.array([2.0, 1.0])\n",
    "\n",
    "    # Run Newton's method\n",
    "    x_opt, f_val, iterations = newton_method(f, grad_f, hess_f, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1. 1.]\n",
      "Function value at minimum: 3.4326461875363225e-20\n",
      "Iterations: 6\n"
     ]
    }
   ],
   "source": [
    "# Define the Rosenbrock function, gradient, and Hessian\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    grad_x0 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n",
    "    grad_x1 = 200 * (x[1] - x[0]**2)\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "def hessian_rosenbrock(x):\n",
    "    hess_00 = 2 - 400 * (x[1] - 3 * x[0]**2)\n",
    "    hess_01 = -400 * x[0]\n",
    "    hess_10 = -400 * x[0]\n",
    "    hess_11 = 200\n",
    "    return np.array([[hess_00, hess_01], [hess_10, hess_11]])\n",
    "\n",
    "# Test Newton's method on the Rosenbrock function\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.2, 1.0])  # A common starting point for testing Rosenbrock\n",
    "\n",
    "    # Run Newton's method\n",
    "    x_opt, f_val, iterations = newton_method(rosenbrock, grad_rosenbrock, hessian_rosenbrock, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [-15. -15.]\n",
      "Function value at minimum: 6.118046410036516e-07\n",
      "Iterations: 14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the exponential test function\n",
    "def exponential_function(x):\n",
    "    \"\"\"\n",
    "    Exponential test function: f(x, y) = exp(x) + exp(y)\n",
    "    \"\"\"\n",
    "    return np.exp(x[0]) + np.exp(x[1])\n",
    "\n",
    "# Define the gradient of the exponential test function\n",
    "def grad_exponential_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the exponential test function: grad(f) = [exp(x), exp(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.exp(x[0])\n",
    "    grad_x1 = np.exp(x[1])\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Define the Hessian of the exponential test function\n",
    "def hess_exponential_function(x):\n",
    "    \"\"\"\n",
    "    Hessian of the exponential test function: diagonal matrix with exp(x) and exp(y)\n",
    "    \"\"\"\n",
    "    hess_x0 = np.exp(x[0])  # Second partial derivative w.r.t. x\n",
    "    hess_x1 = np.exp(x[1])  # Second partial derivative w.r.t. y\n",
    "    return np.array([[hess_x0, 0], [0, hess_x1]])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.0, -1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, `broyden_cg`, and `newton_method` are implemented or imported\n",
    "    x_opt, f_val, iterations = newton_method(exponential_function, grad_exponential_function, hess_exponential_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian is singular or not invertible. Exiting.\n",
      "Optimized x: [1. 1.]\n",
      "Function value at minimum: 0.4546487134128409\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the non-convex test function\n",
    "def non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Non-convex test function: f(x, y) = sin(x) * cos(y)\n",
    "    \"\"\"\n",
    "    return np.sin(x[0]) * np.cos(x[1])\n",
    "\n",
    "# Define the gradient of the non-convex test function\n",
    "def grad_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the non-convex test function:\n",
    "    grad(f) = [cos(x) * cos(y), -sin(x) * sin(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.cos(x[0]) * np.cos(x[1])  # Partial derivative w.r.t. x\n",
    "    grad_x1 = -np.sin(x[0]) * np.sin(x[1])  # Partial derivative w.r.t. y\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Define the Hessian of the non-convex test function\n",
    "def hess_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Hessian of the non-convex test function:\n",
    "    The second derivatives:\n",
    "    H[0,0] = -sin(x) * cos(y)\n",
    "    H[1,1] = -sin(x) * cos(y)\n",
    "    H[0,1] = -cos(x) * sin(y)\n",
    "    H[1,0] = -cos(x) * sin(y)\n",
    "    \"\"\"\n",
    "    hess_x0x0 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. x\n",
    "    hess_x1x1 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. y\n",
    "    hess_x0x1 = -np.cos(x[0]) * np.sin(x[1])  # Mixed partial derivative\n",
    "    hess_x1x0 = hess_x0x1  # Symmetry of the Hessian\n",
    "    return np.array([[hess_x0x0, hess_x0x1], [hess_x1x0, hess_x1x1]])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([1.0, 1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, and `broyden_cg` are implemented or imported\n",
    "    x_opt, f_val, iterations = newton_method(non_convex_function, grad_non_convex_function, hess_non_convex_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.45464871 -0.45464871]\n",
      " [-0.45464871 -0.45464871]]\n"
     ]
    }
   ],
   "source": [
    "print(hess_non_convex_function([1,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
