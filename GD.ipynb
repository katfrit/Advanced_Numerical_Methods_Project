{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Wolfe_line_search import wolfe_line_search\n",
    "\n",
    "def gradient_descent_wolfe(f, grad_f, x0, tol=1e-6, max_iter=4000):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Wolfe Line Search.\n",
    "\n",
    "    Parameters:\n",
    "        f (callable): Objective function to minimize.\n",
    "        grad_f (callable): Gradient of the objective function.\n",
    "        x0 (numpy array): Initial guess.\n",
    "        tol (float): Convergence tolerance.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        x (numpy array): Optimized variable.\n",
    "        f_val (float): Objective function value at the minimum.\n",
    "        iter_count (int): Number of iterations performed.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    iter_count = 0\n",
    "    g = grad_f(x)\n",
    "    while np.linalg.norm(g) > tol and iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "\n",
    "        # Compute the gradient\n",
    "        \n",
    "\n",
    "        # Determine the step size using Wolfe Line Search\n",
    "        p = -g  # Descent direction\n",
    "        alpha = wolfe_line_search(f, grad_f, x, p)\n",
    "\n",
    "        # Update the variable\n",
    "        x = x + alpha * p\n",
    "        g = grad_f(x)\n",
    "\n",
    "    return x, f(x), iter_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1. 2.]\n",
      "Function value at minimum: 0.0\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the function and its gradient\n",
    "    def f(x):\n",
    "        return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "    def grad_f(x):\n",
    "        return np.array([2 * (x[0] - 1), 2 * (x[1] - 2)])\n",
    "    \n",
    "    # Hessian as a NumPy array\n",
    "    def hess_f(x):\n",
    "        return np.array([[2, 0], [0, 2]])  # Identity matrix for the quadratic function\n",
    "\n",
    "    # Initial guess\n",
    "    x0 = np.array([2.0, 1.0])\n",
    "\n",
    "    # Run Newton's method\n",
    "    x_opt, f_val, iterations = gradient_descent_wolfe(f, grad_f, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [0.99850073 0.99700137]\n",
      "Function value at minimum: 2.248363838888242e-06\n",
      "Iterations: 4000\n"
     ]
    }
   ],
   "source": [
    "# Define the Rosenbrock function and its gradient\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    grad_x0 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n",
    "    grad_x1 = 200 * (x[1] - x[0]**2)\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "    # Run the Broyden-CG algorithm with Wolfe line search\n",
    "    x_opt, f_val, iterations = gradient_descent_wolfe(rosenbrock, grad_rosenbrock, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [-14.2999508 -14.2999508]\n",
      "Function value at minimum: 1.2320838678642153e-06\n",
      "Iterations: 87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the exponential test function\n",
    "def exponential_function(x):\n",
    "    \"\"\"\n",
    "    Exponential test function: f(x, y) = exp(x) + exp(y)\n",
    "    \"\"\"\n",
    "    return np.exp(x[0]) + np.exp(x[1])\n",
    "\n",
    "# Define the gradient of the exponential test function\n",
    "def grad_exponential_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the exponential test function: grad(f) = [exp(x), exp(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.exp(x[0])\n",
    "    grad_x1 = np.exp(x[1])\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.0, -1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, and `broyden_cg` are implemented or imported\n",
    "    x_opt, f_val, iterations = gradient_descent_wolfe(exponential_function, grad_exponential_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1.57079633 3.14159265]\n",
      "Function value at minimum: -1.0\n",
      "Iterations: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the non-convex test function\n",
    "def non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Non-convex test function: f(x, y) = sin(x) * cos(y)\n",
    "    \"\"\"\n",
    "    return np.sin(x[0]) * np.cos(x[1])\n",
    "\n",
    "# Define the gradient of the non-convex test function\n",
    "def grad_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the non-convex test function:\n",
    "    grad(f) = [cos(x) * cos(y), -sin(x) * sin(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.cos(x[0]) * np.cos(x[1])  # Partial derivative w.r.t. x\n",
    "    grad_x1 = -np.sin(x[0]) * np.sin(x[1])  # Partial derivative w.r.t. y\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Define the Hessian of the non-convex test function\n",
    "def hess_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Hessian of the non-convex test function:\n",
    "    The second derivatives:\n",
    "    H[0,0] = -sin(x) * cos(y)\n",
    "    H[1,1] = -sin(x) * cos(y)\n",
    "    H[0,1] = -cos(x) * sin(y)\n",
    "    H[1,0] = -cos(x) * sin(y)\n",
    "    \"\"\"\n",
    "    hess_x0x0 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. x\n",
    "    hess_x1x1 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. y\n",
    "    hess_x0x1 = -np.cos(x[0]) * np.sin(x[1])  # Mixed partial derivative\n",
    "    hess_x1x0 = hess_x0x1  # Symmetry of the Hessian\n",
    "    return np.array([[hess_x0x0, hess_x0x1], [hess_x1x0, hess_x1x1]])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([1.0, 1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, and `broyden_cg` are implemented or imported\n",
    "    x_opt, f_val, iterations = gradient_descent_wolfe(non_convex_function, grad_non_convex_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
