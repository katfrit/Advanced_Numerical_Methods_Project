{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Wolfe_line_search import wolfe_line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smabfgs(f, grad_f, x0, tol=1e-6, max_iter=500):\n",
    "    \"\"\"\n",
    "    Implementation of the SMABFGS algorithm as described in the paper.\n",
    "    \n",
    "    Parameters:\n",
    "        f (callable): Objective function to minimize.\n",
    "        grad_f (callable): Gradient of the objective function.\n",
    "        x0 (numpy array): Initial guess.\n",
    "        tol (float): Convergence tolerance.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        x (numpy array): Optimized variable.\n",
    "        f_val (float): Objective function value at the minimum.\n",
    "        iter_count (int): Number of iterations performed.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    g = grad_f(x)\n",
    "    n = len(x0)\n",
    "    I = np.eye(n)  # Identity matrix\n",
    "    theta = 1.0    # Scaling parameter\n",
    "    iter_count = 0\n",
    "\n",
    "    while np.linalg.norm(g) > tol and iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "\n",
    "        # Compute search direction\n",
    "        H = theta * I\n",
    "        p = -H @ g\n",
    "\n",
    "        # Wolfe line search to determine step size\n",
    "        alpha = wolfe_line_search(f, grad_f, x, p)\n",
    "\n",
    "        # Update x\n",
    "        x_new = x + alpha * p\n",
    "\n",
    "        # Update gradient\n",
    "        g_new = grad_f(x_new)\n",
    "\n",
    "        # Compute differences\n",
    "        s = x_new - x\n",
    "        y = g_new - g\n",
    "\n",
    "        # Update scaling parameter Î¸\n",
    "        theta = np.dot(s, y) / np.dot(y, y)\n",
    "\n",
    "        # Compute the rank-one modification term\n",
    "        tau = np.dot(s, y) / np.linalg.norm(s)**2\n",
    "        z = -theta * y + (1 + theta * np.dot(y, y) / np.dot(s, y)) * s\n",
    "        gamma = tau + np.dot(s, y) / (np.linalg.norm(s)**2 + tau * theta * (np.linalg.norm(y)**2 / np.dot(s, y) - np.dot(s, y) / np.linalg.norm(s)**2))\n",
    "\n",
    "        # Augmented Hessian update\n",
    "        H += -tau / gamma * np.outer(z, z) / np.dot(s, y)\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        x = x_new\n",
    "        g = g_new\n",
    "\n",
    "    return x, f(x), iter_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Quadratic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1. 2.]\n",
      "Function value at minimum: 0.0\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the function and its gradient\n",
    "    def f(x):\n",
    "        return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "\n",
    "    def grad_f(x):\n",
    "        return np.array([2 * (x[0] - 1), 2 * (x[1] - 2)])\n",
    "\n",
    "    # Initial guess\n",
    "    x0 = np.array([2.0, 1.0])\n",
    "\n",
    "    # Run the Broyden-CG algorithm with Wolfe line search\n",
    "    x_opt, f_val, iterations = smabfgs(f, grad_f, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenbruck Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [0.99999995 0.99999989]\n",
      "Function value at minimum: 2.8276664658719827e-15\n",
      "Iterations: 213\n"
     ]
    }
   ],
   "source": [
    "# Define the Rosenbrock function and its gradient\n",
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    grad_x0 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n",
    "    grad_x1 = 200 * (x[1] - x[0]**2)\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "    # Run the Broyden-CG algorithm with Wolfe line search\n",
    "    x_opt, f_val, iterations = smabfgs(rosenbrock, grad_rosenbrock, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [-14.63565282 -14.63565282]\n",
      "Function value at minimum: 8.807380048985129e-07\n",
      "Iterations: 20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the exponential test function\n",
    "def exponential_function(x):\n",
    "    \"\"\"\n",
    "    Exponential test function: f(x, y) = exp(x) + exp(y)\n",
    "    \"\"\"\n",
    "    return np.exp(x[0]) + np.exp(x[1])\n",
    "\n",
    "# Define the gradient of the exponential test function\n",
    "def grad_exponential_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the exponential test function: grad(f) = [exp(x), exp(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.exp(x[0])\n",
    "    grad_x1 = np.exp(x[1])\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([-1.0, -1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, and `broyden_cg` are implemented or imported\n",
    "    x_opt, f_val, iterations = smabfgs(exponential_function, grad_exponential_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x: [1.57079608 3.14159239]\n",
      "Function value at minimum: -0.9999999999999368\n",
      "Iterations: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the non-convex test function\n",
    "def non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Non-convex test function: f(x, y) = sin(x) * cos(y)\n",
    "    \"\"\"\n",
    "    return np.sin(x[0]) * np.cos(x[1])\n",
    "\n",
    "# Define the gradient of the non-convex test function\n",
    "def grad_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Gradient of the non-convex test function:\n",
    "    grad(f) = [cos(x) * cos(y), -sin(x) * sin(y)]\n",
    "    \"\"\"\n",
    "    grad_x0 = np.cos(x[0]) * np.cos(x[1])  # Partial derivative w.r.t. x\n",
    "    grad_x1 = -np.sin(x[0]) * np.sin(x[1])  # Partial derivative w.r.t. y\n",
    "    return np.array([grad_x0, grad_x1])\n",
    "\n",
    "# Define the Hessian of the non-convex test function\n",
    "def hess_non_convex_function(x):\n",
    "    \"\"\"\n",
    "    Hessian of the non-convex test function:\n",
    "    The second derivatives:\n",
    "    H[0,0] = -sin(x) * cos(y)\n",
    "    H[1,1] = -sin(x) * cos(y)\n",
    "    H[0,1] = -cos(x) * sin(y)\n",
    "    H[1,0] = -cos(x) * sin(y)\n",
    "    \"\"\"\n",
    "    hess_x0x0 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. x\n",
    "    hess_x1x1 = -np.sin(x[0]) * np.cos(x[1])  # Second partial derivative w.r.t. y\n",
    "    hess_x0x1 = -np.cos(x[0]) * np.sin(x[1])  # Mixed partial derivative\n",
    "    hess_x1x0 = hess_x0x1  # Symmetry of the Hessian\n",
    "    return np.array([[hess_x0x0, hess_x0x1], [hess_x1x0, hess_x1x1]])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initial guess\n",
    "    x0 = np.array([1.0, 1.0])\n",
    "\n",
    "    # Example function calls to hypothetical optimization methods\n",
    "    # Ensure that `smabfgs`, `gradient_descent`, and `broyden_cg` are implemented or imported\n",
    "    x_opt, f_val, iterations = smabfgs(non_convex_function, grad_non_convex_function, x0)\n",
    "\n",
    "    print(f\"Optimized x: {x_opt}\")\n",
    "    print(f\"Function value at minimum: {f_val}\")\n",
    "    print(f\"Iterations: {iterations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
